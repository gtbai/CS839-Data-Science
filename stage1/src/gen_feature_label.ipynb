{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# @Date    : 2019-03-04 17:31:29\n",
    "# @Author  : Bruce Bai (guangtong.bai@wisc.edu)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import timeit\n",
    "import random\n",
    "import nltk\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERED_DOC_DIR = '../filtered_documents/'\n",
    "MAX_EXAMPLE_LEN = 3\n",
    "PREFIX_SUFFIX_LIST_DIR = '../prefix_suffix_lists/'\n",
    "BLACK_WHITE_LIST_DIR = '../black_white_lists/'\n",
    "\n",
    "FEATURE_LIST = [\n",
    "    'contains_mex_char',\n",
    "    'has_extras_in_middle',\n",
    "    \n",
    "    'next_word_verb',\n",
    "    'all_proper_noun',\n",
    "    \n",
    "    'prefix_in_whitelist',\n",
    "    'prefix_in_blacklist',\n",
    "    'suffix_in_whitelist',\n",
    "    'suffix_in_blacklist',\n",
    "    \n",
    "    'surrounded_by_paren',\n",
    "    'has_left_comma',\n",
    "    'has_right_comma',\n",
    "    'has_left_period',\n",
    "    'has_right_period',\n",
    "    'first_last_word_capital',\n",
    "    'surrounding_word_capital',\n",
    "    'all_lowercase',\n",
    "\n",
    "    'end_with_prime_s',\n",
    "#     'tf',\n",
    "#     'df',\n",
    "#     'tf-idf'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_black_set, prefix_white_set = set(), set()\n",
    "suffix_black_set, suffix_white_set = set(), set()\n",
    "\n",
    "# intialize all black list and white list\n",
    "for black_prefix in open(PREFIX_SUFFIX_LIST_DIR + 'prefix_black.txt', 'r').readlines():\n",
    "    prefix_black_set.add(black_prefix.strip().lower())\n",
    "for white_prefix in open(PREFIX_SUFFIX_LIST_DIR + 'prefix_white.txt', 'r').readlines():\n",
    "    prefix_white_set.add(white_prefix.strip().lower())\n",
    "for black_suffix in open(PREFIX_SUFFIX_LIST_DIR + 'suffix_black.txt', 'r').readlines():\n",
    "    suffix_black_set.add(black_suffix.strip().lower())\n",
    "for white_suffix in open(PREFIX_SUFFIX_LIST_DIR + 'suffix_white.txt', 'r').readlines():\n",
    "    suffix_white_set.add(white_suffix.strip().lower())\n",
    "    \n",
    "prefix_suffix_set = set()\n",
    "prefix_suffix_set = prefix_black_set | prefix_white_set | suffix_black_set | suffix_white_set\n",
    "\n",
    "black_set = set()\n",
    "for black_word in open(BLACK_WHITE_LIST_DIR + 'black_list.txt', 'r').readlines():\n",
    "    black_set.add(black_word.strip().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unil Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brackets_matching(example_padded, lbrace, rbrace):\n",
    "    example_len = len(example_padded) - 4\n",
    "    label = 0\n",
    "    left_brace_max_index = -1\n",
    "    for left_index in range(example_len-1, example_len+2):\n",
    "        if lbrace in example_padded[left_index]:\n",
    "            left_brace_max_index = left_index\n",
    "    right_brace_min_index = len(example_padded)\n",
    "    for right_index in range(4, 1, -1):\n",
    "        if rbrace in example_padded[right_index]:\n",
    "            right_brace_min_index = right_index\n",
    "    if (left_brace_max_index > -1 and left_brace_max_index <= 2 and \n",
    "        right_brace_min_index < len(example_padded) and right_brace_min_index >= example_len+1):\n",
    "        label = 1\n",
    "    for left_index in range(example_len-1, example_len+1):\n",
    "        if rbrace in example_padded[left_index] and left_index >= left_brace_max_index:\n",
    "            label = 0\n",
    "            break\n",
    "    for right_index in range(4, 2, -1):\n",
    "        if lbrace in example_padded[right_index] and right_index <= right_brace_min_index:\n",
    "            label = 0\n",
    "            break\n",
    "    return label\n",
    "\n",
    "def has_surrounded_symbol(example_padded, pos, symbol):\n",
    "    example_len = len(example_padded) - 4\n",
    "    if pos == 'left':\n",
    "        if example_padded[1][-1] == symbol:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if example_padded[example_len+1][-1] == symbol:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def remove_extras(s):\n",
    "    if s in prefix_white_set or s in suffix_white_set:\n",
    "        return s\n",
    "    if s[-2:] == '\\'s':\n",
    "        s = s[:-2]\n",
    "    s = re.sub('[^a-zA-Z]', '', s)\n",
    "    return s\n",
    "\n",
    "def gen_word_prop_dict(text):\n",
    "    word_tag_dict = dict()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    word_tags = nltk.pos_tag(words)\n",
    "    for word, tag in word_tags:\n",
    "        if word not in word_tag_dict:\n",
    "            word_tag_dict[word] = set(tag)\n",
    "        else:\n",
    "            word_tag_dict[word].add(tag)\n",
    "    return word_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "label = brackets_matching(['father,', 'Dr.', '{Henry', 'Jones}.', 'The'], '{', '}')\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature and Label Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature matrix and label vector for a document and a particular example length\n",
    "def gen_feature_label_example_len(text, example_len, word_tag_dict):\n",
    "#     X_len = pd.DataFrame(columns=(['example'] + FEATURE_LIST))\n",
    "    X_len = pd.DataFrame()\n",
    "    y_len = pd.DataFrame(columns=['example', 'is_person_name'])\n",
    "    parts = text.split(' ')\n",
    "    index = 2\n",
    "    while index+example_len+2 <= len(parts):\n",
    "        example_padded = parts[index-2:index+example_len+2]\n",
    "        example = example_padded[2:2 + example_len]\n",
    "        example_joined = ' '.join(example)\n",
    "        feature_dict = {'example': example_joined}\n",
    "        \n",
    "        # ========================================================================\n",
    "        # \"example_padded\" has the following form:                              ||\n",
    "        # [pad_0, pad_1, word_1, ..., word_n,                 pad_-2, pad_-1]   ||\n",
    "        #  0      1      2            len-3 (example_len+1)   len-2   len-1     ||\n",
    "        # ========================================================================\n",
    "        \n",
    "        # generate \"surrounded_by_paren\" feature\n",
    "        feature_dict['surrounded_by_paren'] = brackets_matching(example_padded, '(', ')')\n",
    "\n",
    "        # generate \"has_left_comma\" feature\n",
    "        feature_dict['has_left_comma'] = has_surrounded_symbol(example_padded, 'left', ',')\n",
    "        \n",
    "        # generate \"has_right_comma\" feature\n",
    "        feature_dict['has_right_comma'] = has_surrounded_symbol(example_padded, 'right', ',')\n",
    "        \n",
    "        # generate \"has_left_period\" feature\n",
    "        feature_dict['has_left_period'] = has_surrounded_symbol(example_padded, 'left', '.')\n",
    "        \n",
    "        # generate \"has_right_period\" feature\n",
    "        feature_dict['has_right_period'] = has_surrounded_symbol(example_padded, 'right', '.')\n",
    "        \n",
    "        # generate \"all_lowercase\" feature\n",
    "        feature_dict['all_lowercase'] = 1 if re.fullmatch(r'[^A-Z]+', example_joined) else 0\n",
    "        \n",
    "        # generate \"all_uppercase\" feature\n",
    "        feature_dict['all_uppercase'] = 1 if re.fullmatch(r'[^a-z]+', example_joined) else 0\n",
    "        \n",
    "        # generate \"prefix_in_whitelist\" feature\n",
    "        feature_dict['prefix_in_whitelist'] = 1 if remove_extras(example_padded[1]) in prefix_white_set else 0\n",
    "        \n",
    "        # generate \"prefix_in_blacklist\" feature\n",
    "        feature_dict['prefix_in_blacklist'] = 1 if (remove_extras(example_padded[1])).lower() in prefix_black_set else 0\n",
    "        \n",
    "        # generate \"suffix_in_whitelist\" feature\n",
    "        feature_dict['suffix_in_whitelist'] = 1 if remove_extras(example_padded[2 + example_len]) in suffix_white_set else 0\n",
    "        \n",
    "        # generate \"suffix_in_blacklist\" feature\n",
    "        feature_dict['suffix_in_blacklist'] = 1 if (remove_extras(example_padded[2 + example_len])).lower() in suffix_black_set else 0\n",
    "        \n",
    "        # generate \"end_with_prime_s\" feature\n",
    "#         feature_dict['end_with_prime_s'] = 1 if (re.fullmatch('.*\\'s', example_padded[1+example_len])) else 0\n",
    "        \n",
    "        # generate \"first_last_word_capital\" feature\n",
    "        # TODO change to all_word_capotal\n",
    "        first_capital = re.fullmatch('[^a-zA-Z]*[A-Z].*', example_padded[2])\n",
    "        last_capital = re.fullmatch('[^a-zA-Z]*[A-Z].*', example_padded[1+example_len])\n",
    "        feature_dict['first_last_word_capital'] = 1 if (first_capital and last_capital) else 0\n",
    "        \n",
    "        # generate \"surrounding_word_capital\" feature\n",
    "#         left_capital = re.fullmatch('[^a-zA-Z]*[A-Z].*', example_padded[1])\n",
    "#         right_capital = re.fullmatch('[^a-zA-Z]*[A-Z].*', example_padded[2+example_len])\n",
    "#         feature_dict['surrounding_word_capital'] = 1 if (left_capital or right_capital) else 0\n",
    "        \n",
    "        # generate \"next_word_verb\" feautre, including be-verb\n",
    "        feature_dict['next_word_verb'] = 0\n",
    "        next_word = remove_extras(example_padded[2 + example_len])\n",
    "        if next_word in word_tag_dict:\n",
    "            for tag in word_tag_dict[next_word]:\n",
    "                if tag.startswith('V'):\n",
    "                    feature_dict['next_word_verb'] = 1\n",
    "                    break\n",
    "\n",
    "        # generate \"all_noun\" feautre\n",
    "        feature_dict['all_noun'] = 1\n",
    "        for word in example:\n",
    "            word = remove_extras(word)\n",
    "            if word not in word_tag_dict:\n",
    "                feature_dict['all_noun'] = 0\n",
    "                break\n",
    "            can_be_noun = False\n",
    "            for tag in word_tag_dict[word]:\n",
    "                if tag.startswith('N'):\n",
    "                    can_be_noun = True\n",
    "                    break\n",
    "            if not can_be_noun:\n",
    "                feature_dict['all_noun'] = 0\n",
    "                break\n",
    "                    \n",
    "        # generata \"contains_proper_noun\" feature\n",
    "        feature_dict['contains_proper_noun'] = 0\n",
    "        for word in example:\n",
    "            word = remove_extras(word)\n",
    "            if word in word_tag_dict and 'NNP' in word_tag_dict[word]:\n",
    "                feature_dict['contains_proper_noun'] = 1\n",
    "                break\n",
    "        \n",
    "        # generata \"contains_extras_in_middle\" feature\n",
    "        feature_dict['contains_extras_in_middle'] = 0\n",
    "        middle_chars = example_joined[2:-2]\n",
    "        if re.search(r'[^a-zA-Z\\s]', middle_chars):\n",
    "            feature_dict['contains_extras_in_middle'] = 1\n",
    "        \n",
    "        # generata \"contains_amazing_char\" feature\n",
    "        feature_dict['contains_amazing_char'] = 0 \n",
    "        if re.search(r'[óéöäûâ]', example_joined):\n",
    "            feature_dict['contains_amazing_char'] = 1\n",
    "        \n",
    "#         # generata \"contains_prefix_suffix\" feature\n",
    "#         feature_dict['contains_prefix_suffix'] = 0\n",
    "#         for word in example:\n",
    "#             for some_fix in prefix_suffix_set:\n",
    "#                 if some_fix in word:\n",
    "#                     feature_dict['contains_prefix_suffix'] = 1\n",
    "#                     break\n",
    "\n",
    "        # generata \"surrounding_word_and\" feature\n",
    "        feature_dict['surrounding_word_and'] = 0\n",
    "        if example_padded[1] == 'and' or example_padded[-2] == 'and':\n",
    "            feature_dict['surrounding_word_and'] = 1 \n",
    "        \n",
    "        # generate \"in_blacklist\" feature\n",
    "        feature_dict['in_blacklist'] = 0\n",
    "        for word in example:\n",
    "            word = (remove_extras(word)).lower()\n",
    "            if word in black_set or word in prefix_suffix_set:\n",
    "                feature_dict['in_blacklist'] = 1\n",
    "                break\n",
    "        \n",
    "        # generate \"surrounding_black_word\" feature\n",
    "        feature_dict['surrounding_black_word'] = 0\n",
    "        if example_padded[1] in black_set or example_padded[1] in prefix_suffix_set:\n",
    "            feature_dict['surrounding_black_word'] = 1\n",
    "        if example_padded[-2] in black_set or example_padded[-2] in prefix_suffix_set:\n",
    "            feature_dict['surrounding_black_word'] = 1\n",
    "        \n",
    "        # generate \"tf\" feature\n",
    "        \n",
    "        # generate \"idf\" feature\n",
    "        \n",
    "        # generate \"tf-idf\" feature\n",
    "\n",
    "        \n",
    "        X_len = X_len.append(feature_dict, ignore_index=True)\n",
    "\n",
    "        # generate label\n",
    "        label = brackets_matching(example_padded, '{', '}')\n",
    "        y_len = y_len.append({'example': example_joined, 'is_person_name': label}, ignore_index = True)\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    return X_len, y_len\n",
    "\n",
    "# Generate feature matrix and label vector for a document\n",
    "def gen_feature_label_doc(doc_name):\n",
    "#     X_doc = pd.DataFrame(columns=(['example'] + FEATURE_LIST))\n",
    "    X_doc = pd.DataFrame()\n",
    "    y_doc = pd.DataFrame(columns=['example', 'is_person_name'])\n",
    "    doc = open(FILTERED_DOC_DIR+doc_name, 'r')\n",
    "    text = ' '.join(doc.readlines()[2:]) # skip the title and empty line\n",
    "    word_tag_dict = gen_word_prop_dict(text)\n",
    "    \n",
    "    text = '. . ' + text + ' . .' # pad with '. .' at both ends\n",
    "    for example_len in range(1, MAX_EXAMPLE_LEN+1):\n",
    "        X_len, y_len = gen_feature_label_example_len(text, example_len, word_tag_dict)\n",
    "        X_doc = X_doc.append(X_len, ignore_index=True)\n",
    "        y_doc = y_doc.append(y_len, ignore_index=True)\n",
    "    return X_doc, y_doc\n",
    "\n",
    "# Generate train/test feature matrix and label vector, given a list of documents\n",
    "def gen_feature_label(doc_list):\n",
    "    X = pd.DataFrame()\n",
    "    y = pd.DataFrame(columns=['example', 'is_person_name'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    for doc_name in doc_list:\n",
    "        if doc_name == '.DS_Store':\n",
    "            continue\n",
    "        X_doc, y_doc = gen_feature_label_doc(doc_name)\n",
    "        X = X.append(X_doc, ignore_index=True)\n",
    "        y = y.append(y_doc, ignore_index=True)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature and Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used:  437.873375977\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "# documents are unordered\n",
    "doc_list = os.listdir(FILTERED_DOC_DIR)\n",
    "\n",
    "# documents are ordered\n",
    "# doc_list = sorted(os.listdir(FILTERED_DOC_DIR), key = lambda x: int(x.split('.')[0]))\n",
    "\n",
    "doc_list = doc_list[:300]\n",
    "\n",
    "cutoff = int(0.66 * len(doc_list))\n",
    "\n",
    "train_doc_list = doc_list[:cutoff]\n",
    "test_doc_list = doc_list[cutoff:]\n",
    "\n",
    "X_train, y_train = gen_feature_label(train_doc_list)\n",
    "X_test, y_test = gen_feature_label(test_doc_list)\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print(\"Time used: \", stop - start)\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     print(X.head(1000), y.head(1000))\n",
    "# X.head(1000)\n",
    "# y.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     features      coef\n",
      "0   all_lowercase             -3.523935\n",
      "1   all_noun                   1.541730\n",
      "2   all_uppercase             -2.671647\n",
      "3   contains_amazing_char      3.601959\n",
      "4   contains_extras_in_middle -3.261309\n",
      "5   contains_proper_noun       1.202537\n",
      "6   end_with_prime_s           2.962056\n",
      "7   first_last_word_capital    4.866364\n",
      "8   has_left_comma             0.610259\n",
      "9   has_left_period           -1.552587\n",
      "10  has_right_comma            0.508272\n",
      "11  has_right_period           0.192217\n",
      "12  in_blacklist              -5.216518\n",
      "13  next_word_verb             0.310800\n",
      "14  prefix_in_blacklist       -2.727021\n",
      "15  prefix_in_whitelist        1.261642\n",
      "16  suffix_in_blacklist       -0.811194\n",
      "17  suffix_in_whitelist        0.775501\n",
      "18  surrounded_by_paren        3.758533\n",
      "19  surrounding_black_word    -0.494785\n",
      "20  surrounding_word_and       0.607236\n",
      "Precision: 83.36%, Recall: 88.67%\n",
      "\n",
      "=============================================\n",
      "Test False Positive: \n",
      "                                 example is_person_name  predicted_label\n",
      "170    Lies                               0              1              \n",
      "1810   Administrative                     0              1              \n",
      "1811   Affairs.                           0              1              \n",
      "1832   Permanent                          0              1              \n",
      "1865   Administrative Affairs.            0              1              \n",
      "1974   Barrier                            0              1              \n",
      "1975   Reef                               0              1              \n",
      "2115   Barrier Reef                       0              1              \n",
      "2419   Gunnery                            0              1              \n",
      "2438   Stars                              0              1              \n",
      "2440   Stripes,                           0              1              \n",
      "2559   Stars and Stripes,                 0              1              \n",
      "2593   Commonwealth                       0              1              \n",
      "2594   Games,                             0              1              \n",
      "2641   Commonwealth Games,                0              1              \n",
      "2729   Mixed                              0              1              \n",
      "2731   Arts                               0              1              \n",
      "2732   (MMA)                              0              1              \n",
      "3173   But when {Brendan}'s               0              1              \n",
      "3891   Investigation                      0              1              \n",
      "4878   While                              0              1              \n",
      "4973   Bomb\":                             0              1              \n",
      "5394   Monsters,                          0              1              \n",
      "5395   Inc.                               0              1              \n",
      "5490   Boo                                0              1              \n",
      "5511   Mr.                                0              1              \n",
      "5520   Monsters,                          0              1              \n",
      "5521   Inc.                               0              1              \n",
      "5792   {Mike} and {Sulley}'s              0              1              \n",
      "5965   {Skeeter} ({Stone})                0              1              \n",
      "6014   ({Davis}), {Skeeter}'s             0              1              \n",
      "6037   Despite {Skeeter}'s                0              1              \n",
      "6166   {Aibileen} ({Davis}), {Skeeter}'s  0              1              \n",
      "6267   Kong,                              0              1              \n",
      "6284   At                                 0              1              \n",
      "6344   Hurt                               0              1              \n",
      "8716   Wall                               0              1              \n",
      "9521   Parks                              0              1              \n",
      "9523   Recreation                         0              1              \n",
      "9840   Knightmare                         0              1              \n",
      "9841   Frames.                            0              1              \n",
      "9848   Area                               0              1              \n",
      "9952   Geass,                             0              1              \n",
      "9967   Britannia's                        0              1              \n",
      "9971   Knightmare,                        0              1              \n",
      "9977   Zero                               0              1              \n",
      "10001  Knightmare Frames.                 0              1              \n",
      "10405  Dr.                                0              1              \n",
      "10421  Thirteen                           0              1              \n",
      "10438  Love.                              0              1              \n",
      "10776  Ma                                 0              1              \n",
      "10815  Ma                                 0              1              \n",
      "10820  Ma                                 0              1              \n",
      "10863  Ma's                               0              1              \n",
      "11178  Pictures'                          0              1              \n",
      "11256  Monumental's                       0              1              \n",
      "11333  Monumental's                       0              1              \n",
      "11486  Brown}, {Don}'s                    0              1              \n",
      "11644  {Cosmo Brown}, {Don}'s             0              1              \n",
      "11672  Honor                              0              1              \n",
      "12088  Staff                              0              1              \n",
      "12817  Side,                              0              1              \n",
      "13638  Groundhog                          0              1              \n",
      "13861  Gang.                              0              1              \n",
      "13923  \"Let's                             0              1              \n",
      "14156  Poets                              0              1              \n",
      "14425  Live                               0              1              \n",
      "14426  Aid.                               0              1              \n",
      "14557  Live Aid.                          0              1              \n",
      "14612  {Freddie Mercury}. {Freddie}       0              1              \n",
      "14737  Nostromo                           0              1              \n",
      "15080  While                              0              1              \n",
      "15512  Humans\",                           0              1              \n",
      "16201  Imperial                           0              1              \n",
      "16204  Engineering,                       0              1              \n",
      "16431  Rangers,                           0              1              \n",
      "16685  Boardwalk                          0              1              \n",
      "17231  Whale                              0              1              \n",
      "17236  \"Hunter\"                           0              1              \n",
      "17254  \"Hunter,\"                          0              1              \n",
      "17277  Hunter                             0              1              \n",
      "17291  Examination,                       0              1              \n",
      "17312  Examination,                       0              1              \n",
      "17317  Hunter                             0              1              \n",
      "17966  Enterprise,                        0              1              \n",
      "17976  Trek                               0              1              \n",
      "17988  Enterprise                         0              1              \n",
      "18053  Enterprise NCC                     0              1              \n",
      "18291  Stones                             0              1              \n",
      "18427  Pulitzer                           0              1              \n",
      "18823  Great                              0              1              \n",
      "19869  Show.                              0              1              \n",
      "20088  King's                             0              1              \n",
      "20846  Told                               0              1              \n",
      "21520  Falcon.                            0              1              \n",
      "21801  \"Black                             0              1              \n",
      "21802  Mirror\"                            0              1              \n",
      "21840  \"Black Mirror\"                     0              1              \n",
      "21957  Normal,                            0              1              \n",
      "21960  Weird.                             0              1              \n",
      "22420  Cosmos.                            0              1              \n",
      "22644  Reformed                           0              1              \n",
      "22881  Deadly                             0              1              \n",
      "22882  Sins.                              0              1              \n",
      "22950  Deadly                             0              1              \n",
      "22951  Sins                               0              1              \n",
      "23013  Deadly Sins.                       0              1              \n",
      "23082  Deadly Sins                        0              1              \n",
      "23281  Howl's                             0              1              \n",
      "23386  {Karishifâ}. Seeing                0              1              \n",
      "23471  {Sophie} meets {Howl}'s            0              1              \n",
      "23500  {Howl}, then {Karushifâ}           0              1              \n",
      "24594  Consolidated                       0              1              \n",
      "24595  Life,                              0              1              \n",
      "24656  Central                            0              1              \n",
      "24671  Mssrs.                             0              1              \n",
      "24767  Consolidated Life,                 0              1              \n",
      "25101  Big                                0              1              \n",
      "25171  Kid',                              0              1              \n",
      "25427  Dissatisfied with {Bill}'s         0              1              \n",
      "25583  Lime's                             0              1              \n",
      "25914  Det.                               0              1              \n",
      "26013  Det.                               0              1              \n",
      "26170  Det. {McNulty},                    0              1              \n",
      "27264  {Agâh} and {Nevra}                 0              1              \n",
      "=============================================\n",
      "Train False Positive: \n",
      "                             example is_person_name  predicted_label\n",
      "212    Emphasis                       0              1              \n",
      "408    Show                           0              1              \n",
      "825    Ring.                          0              1              \n",
      "990    Scout                          0              1              \n",
      "992    Russel                         0              1              \n",
      "1693   Green                          0              1              \n",
      "1694   Book,                          0              1              \n",
      "1702   America's                      0              1              \n",
      "1735   America's                      0              1              \n",
      "1839   Green Book,                    0              1              \n",
      "2102   Everything                     0              1              \n",
      "2133   {Stan}, {Kyle},                0              1              \n",
      "2193   {Stan}, {Kyle}, {Cartman},     0              1              \n",
      "2507   \"Silicon                       0              1              \n",
      "2529   Startup.                       0              1              \n",
      "2668   {Saurabh Mandal} ({Mandal}),   0              1              \n",
      "3296   Life\"                          0              1              \n",
      "4470   Water                          0              1              \n",
      "4951   Series,                        0              1              \n",
      "6381   Democratic                     0              1              \n",
      "6625   \"Breaking                      0              1              \n",
      "6626   Bad\"                           0              1              \n",
      "6662   \"Breaking Bad\"                 0              1              \n",
      "7348   B.C.)                          0              1              \n",
      "7736   Ducard} and {Ra's              0              1              \n",
      "7861   Mr.                            0              1              \n",
      "7871   Bear                           0              1              \n",
      "8590   Public                         0              1              \n",
      "8591   Broadcasting                   0              1              \n",
      "8681   Venus                          0              1              \n",
      "8750   Public Broadcasting            0              1              \n",
      "8819   Dr. {Sagan}'s                  0              1              \n",
      "9096   Mob                            0              1              \n",
      "9573   Blue                           0              1              \n",
      "9586   Blue                           0              1              \n",
      "9839   Reconstruction.                0              1              \n",
      "9857   Visuals                        0              1              \n",
      "10427  Grail.                         0              1              \n",
      "10449  Dr.                            0              1              \n",
      "10590  Inept                          0              1              \n",
      "11802  Table                          0              1              \n",
      "11843  Grail.                         0              1              \n",
      "11892  Python                         0              1              \n",
      "12534  Yard's                         0              1              \n",
      "12680  \"Serenity\".                    0              1              \n",
      "12777  Washburne}; {Zoe}'s            0              1              \n",
      "12864  Alleyne Washburne}; {Zoe}'s    0              1              \n",
      "13058  McKinley                       0              1              \n",
      "15117  Hill                           0              1              \n",
      "15247  Blinders                       0              1              \n",
      "15279  Productions,                   0              1              \n",
      "15280  Screen                         0              1              \n",
      "15283  Tiger                          0              1              \n",
      "15284  Aspect                         0              1              \n",
      "15285  Productions,                   0              1              \n",
      "15294  Screen                         0              1              \n",
      "15304  Content                        0              1              \n",
      "15317  Content                        0              1              \n",
      "15338  Peaky Blinders                 0              1              \n",
      "15375  Tiger Aspect                   0              1              \n",
      "15376  Aspect Productions,            0              1              \n",
      "15466  Tiger Aspect Productions,      0              1              \n",
      "16056  Four                           0              1              \n",
      "16057  Fingers'                       0              1              \n",
      "16059  'Bullet                        0              1              \n",
      "16060  Tooth                          0              1              \n",
      "16189  Four Fingers'                  0              1              \n",
      "16192  'Bullet Tooth                  0              1              \n",
      "17249  Ring                           0              1              \n",
      "17277  Ring                           0              1              \n",
      "17283  Ring                           0              1              \n",
      "17299  Doom                           0              1              \n",
      "17361  Ring                           0              1              \n",
      "17727  Philosopher's                  0              1              \n",
      "17728  Stone                          0              1              \n",
      "18044  Visceral                       0              1              \n",
      "19025  Bride\"                         0              1              \n",
      "19988  Limited                        0              1              \n",
      "20046  Mt.                            0              1              \n",
      "20374  Four,                          0              1              \n",
      "20431  Seven,                         0              1              \n",
      "21630  Mrs.                           0              1              \n",
      "21863  Twelve                         0              1              \n",
      "21864  Oaks.                          0              1              \n",
      "21955  {Melanie}. Mammy               0              1              \n",
      "21966  Twelve Oaks.                   0              1              \n",
      "22106  Ones\"                          0              1              \n",
      "22114  Dr.                            0              1              \n",
      "22737  Milton.                        0              1              \n",
      "22827  Guard                          0              1              \n",
      "23533  Williams}). {Cole}'s           0              1              \n",
      "23625  ({Olivia Williams}). {Cole}'s  0              1              \n",
      "24491  India's                        0              1              \n",
      "24847  Rochelle,                      0              1              \n",
      "24854  Jr.                            0              1              \n",
      "24923  Am                             0              1              \n",
      "25490  Wind                           0              1              \n",
      "26690  Craggy                         0              1              \n",
      "26738  Craggy                         0              1              \n",
      "26924  Side                           0              1              \n",
      "26981  {Frank}. {Fiona},              0              1              \n",
      "27160  Goblin's                       0              1              \n",
      "27203  Reaper                         0              1              \n",
      "27762  Rabelaisian                    0              1              \n",
      "27798  Sick                           0              1              \n",
      "28226  Infinity                       0              1              \n",
      "28227  Stones,                        0              1              \n",
      "28311  Infinity Stones,               0              1              \n",
      "28447  Emperor,                       0              1              \n",
      "28815  What                           0              1              \n",
      "28826  As                             0              1              \n",
      "29088  Sue                            0              1              \n",
      "29194  {DJ}. {Sonia}'s                0              1              \n",
      "30434  Brainerd,                      0              1              \n",
      "30757  Shutter                        0              1              \n",
      "34266  Cofi.                          0              1              \n",
      "35535  \"Rosebud\".                     0              1              \n",
      "35743  Knight,                        0              1              \n",
      "35867  Good)                          0              1              \n",
      "35884  Bad)                           0              1              \n",
      "35911  Ugly)                          0              1              \n",
      "36539  Jr.                            0              1              \n",
      "36923  {Scout}. {Atticus Finch}       0              1              \n",
      "37279  Quite                          0              1              \n",
      "37280  Interesting                    0              1              \n",
      "37315  Quite Interesting              0              1              \n",
      "38390  Gran                           0              1              \n",
      "38679  Angel,                         0              1              \n",
      "38698  NERV's                         0              1              \n",
      "38706  Dogma                          0              1              \n",
      "38722  Instrumentality                0              1              \n",
      "38723  Project.                       0              1              \n",
      "38793  Instrumentality Project.       0              1              \n",
      "39261  Nevertheless,                  0              1              \n",
      "39860  Boot\"                          0              1              \n",
      "39870  U-Boat,                        0              1              \n",
      "39946  \"Das Boot\"                     0              1              \n",
      "40082  \"Spotlight\"                    0              1              \n",
      "40107  Boston's                       0              1              \n",
      "40217  Formula                        0              1              \n",
      "42189  Did                            0              1              \n",
      "42370  Culture                        0              1              \n",
      "43471  Jr.                            0              1              \n",
      "44170  Suite                          0              1              \n",
      "44172  Op.                            0              1              \n",
      "44415  Yes                            0              1              \n",
      "44428  Cabinet                        0              1              \n",
      "44441  As                             0              1              \n",
      "44723  Downton                        0              1              \n",
      "45015  Gallifrey,                     0              1              \n",
      "45031  Relative                       0              1              \n",
      "45032  Dimension                      0              1              \n",
      "45095  Relative Dimension             0              1              \n",
      "45233  \"Duchess.\"                     0              1              \n",
      "45827  God's                          0              1              \n",
      "46502  End                            0              1              \n",
      "47766  Begins,                        0              1              \n",
      "48017  Berk,                          0              1              \n",
      "48058  Regardless,                    0              1              \n",
      "48089  Toothless,                     0              1              \n",
      "48407  Television                     0              1              \n",
      "48408  Network                        0              1              \n",
      "48478  Television Network             0              1              \n",
      "48547  UBS Television Network         0              1              \n",
      "48985  Count                          0              1              \n",
      "49276  Years                          0              1              \n",
      "49750  White                          0              1              \n",
      "51370  Security                       0              1              \n",
      "52781  {Harry} ({Jared                0              1              \n",
      "52908  {Harry} ({Jared Leto})         0              1              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gtbai/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/gtbai/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X_train_no_example = X_train.drop(['example'], axis=1).astype('int')\n",
    "X_test_no_example = X_test.drop(['example'], axis=1).astype('int')\n",
    "\n",
    "y_train_no_example = y_train['is_person_name'].astype('int')\n",
    "y_test_no_example = y_test['is_person_name'].astype('int')\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "# clf = RandomForestClassifier()\n",
    "clf.fit(X_train_no_example, y_train_no_example)\n",
    "\n",
    "if isinstance(clf, LogisticRegression):\n",
    "    coef_df = pd.DataFrame()\n",
    "    coef_df['features'] = X_train_no_example.columns\n",
    "    coef_df['coef'] = clf.coef_[0]\n",
    "    print(coef_df)\n",
    "\n",
    "y_predict = clf.predict(X_test_no_example)\n",
    "\n",
    "\n",
    "\n",
    "precision = precision_score(y_test_no_example, y_predict)\n",
    "recall = recall_score(y_test_no_example, y_predict)\n",
    "\n",
    "print(\"Precision: {:.2f}%, Recall: {:.2f}%\\n\".format(precision*100, recall*100))\n",
    "\n",
    "X_test[np.not_equal(y_test_no_example, y_predict)].head(100)\n",
    "\n",
    "# print(X_test[np.not_equal(y_test_no_example, y_predict)])\n",
    "# print(X_test[np.not_equal(y_test_no_example, y_predict)])\n",
    "\n",
    "y_false = y_test[np.not_equal(y_test_no_example, y_predict)]\n",
    "X_false = X_test[np.not_equal(y_test_no_example, y_predict)]\n",
    "y_false['predicted_label'] = y_predict[np.not_equal(y_test_no_example, y_predict)]\n",
    "\n",
    "print('=============================================')\n",
    "print('Test False Positive: ')\n",
    "print(y_false[y_false['predicted_label'] == 1])\n",
    "# print(X_false[y_false['predicted_label'] == 1])\n",
    "\n",
    "# print('=============================================')\n",
    "# print('Test False Negative: ')\n",
    "# print(y_compare[y_compare['predicted_label'] == 0])\n",
    "\n",
    "y_predict_train = clf.predict(X_train_no_example)\n",
    "y_false_train = y_train[np.not_equal(y_train_no_example, y_predict_train)]\n",
    "y_false_train['predicted_label'] = y_predict_train[np.not_equal(y_train_no_example, y_predict_train)]\n",
    "\n",
    "print('=============================================')\n",
    "print('Train False Positive: ')\n",
    "print(y_false_train[y_false_train['predicted_label'] == 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
